{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f65fb2-39a4-4f2b-bcd4-66fe844bbe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "import importlib\n",
    "import inspect\n",
    "import pprint\n",
    "import os\n",
    "from pathlib import Path\n",
    "#\n",
    "from Pegasus.api import *\n",
    "pegasus_graphviz = importlib.import_module(\"pegasus-graphviz\")\n",
    "\n",
    "import graphviz as graphviz\n",
    "import pygraphviz as pgv\n",
    "from IPython.display import Image\n",
    "from IPython.display import IFrame\n",
    "from wand.image import Image as WImage\n",
    "\n",
    "# splinter\n",
    "import subprocess\n",
    "import time\n",
    "import concurrent.futures\n",
    "import slurm_splinter\n",
    "\n",
    "#\n",
    "import yaml as yaml\n",
    "import pydot as pydot\n",
    "\n",
    "import itertools\n",
    "import copy\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import ast\n",
    "from PIL import ImageColor\n",
    "import ipyplot\n",
    "\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import logging\n",
    "\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "BINARY_PATH  ='/home/scitech/shared-data/maestro-test/binaries/'\n",
    "MOCKTAGE_PATH='/home/scitech/mocktage/build/bin/'\n",
    "DATA_PATH    ='/home/scitech/shared-data/maestro-test/binaries/data/'\n",
    "SCRATCH_PATH ='/home/scitech/scratch/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478bd82a-73b0-46c8-9acd-fc07f338ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformation_catalog(wf):\n",
    "    \n",
    "    tc = TransformationCatalog()\n",
    "    trans = {}\n",
    "\n",
    "    exes = {}\n",
    "    binary_paths = [BINARY_PATH, MOCKTAGE_PATH]\n",
    "    \n",
    "    for base in binary_paths:\n",
    "        base_dir = os.path.dirname(base)\n",
    "\n",
    "        for fname in os.listdir(base_dir):\n",
    "            #print('Making Transformation for', fname)\n",
    "            transformation = None\n",
    "            if fname[0] == '.':\n",
    "                continue\n",
    "\n",
    "            transformation = Transformation(fname, \n",
    "                                            site='local',\n",
    "                                            pfn=os.path.join(base_dir, fname), \n",
    "                                            is_stageable=True)\n",
    "            transformation.add_env(PATH='/usr/bin:/bin:.')\n",
    "\n",
    "            # memory requirement\n",
    "            transformation.add_profiles(Namespace.CONDOR, 'request_memory', '1 GB')\n",
    "\n",
    "            # some transformations can be clustered for effiency\n",
    "            #if fname in ['gmProject', 'mDiff', 'mDiffFit', 'mBackground']:\n",
    "            #    transformation.add_profiles(Namespace.PEGASUS, 'clusters.size', '3')\n",
    "\n",
    "            # keep a handle to added ones, for use later\n",
    "            trans[fname] = transformation\n",
    "\n",
    "            tc.add_transformations(transformation)\n",
    "\n",
    "    wf.add_transformation_catalog(tc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13706251-ba20-494e-8400-a1c5527c9c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_site_catalog():\n",
    "    # create a SiteCatalog object\n",
    "    sc = SiteCatalog()\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # create a \"local\" site\n",
    "    local = Site(\"local\", arch=Arch.X86_64, os_type=OS.LINUX)\n",
    "\n",
    "    #pprint(dir(Directory))\n",
    "    # create and add a shared scratch and local storage directories to the site \"local\"\n",
    "    local_shared_scratch_dir = Directory(Directory.SHARED_SCRATCH, path=SCRATCH_PATH)\\\n",
    "        .add_file_servers(FileServer(\"file://\" + SCRATCH_PATH, Operation.ALL))\n",
    "\n",
    "    #local_local_storage_dir = Directory(Directory.LOCAL_STORAGE, path=\"/tmp/pegasus/local\")\\\n",
    "    #                            .add_file_servers(FileServer(\"file:///tmp/pegasus/local\", Operation.ALL))\n",
    "    local_shared_binary_dir = Directory(Directory.LOCAL_STORAGE, path=BINARY_PATH)\\\n",
    "        .add_file_servers(FileServer(\"file://\" + BINARY_PATH, Operation.ALL))\n",
    "    local_shared_binary_dir = Directory(Directory.LOCAL_STORAGE, path=MOCKTAGE_PATH)\\\n",
    "        .add_file_servers(FileServer(\"file://\" + MOCKTAGE_PATH, Operation.ALL))\n",
    "\n",
    "    local.add_directories(local_shared_scratch_dir, local_shared_binary_dir)\n",
    "\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    # create a \"condorpool\" site\n",
    "    condorpool = Site(\"condorpool\")\\\n",
    "                    .add_pegasus_profile(style=\"condor\")\\\n",
    "                    .add_pegasus_profile(auxillary_local=\"true\")\\\n",
    "                    .add_condor_profile(universe=\"local\")\n",
    "\n",
    "    # create and add a shared scratch directory to the site \"condorpool\"\n",
    "    condorpool_shared_scratch_dir = Directory(Directory.SHARED_SCRATCH, path=SCRATCH_PATH)\\\n",
    "        .add_file_servers(FileServer(\"file://\" + SCRATCH_PATH, Operation.ALL))\n",
    "#     condorpool_local_storage_dir = Directory(Directory.LOCAL_STORAGE, path=SCRATCH_PATH)\\\n",
    "#         .add_file_servers(FileServer(\"file://\" + SCRATCH_PATH, Operation.ALL))\n",
    "    condorpool_shared_binary_dir = Directory(Directory.LOCAL_STORAGE, path=BINARY_PATH)\\\n",
    "        .add_file_servers(FileServer(\"file://\" + BINARY_PATH, Operation.ALL))\n",
    "    condorpool_shared_binary_dir = Directory(Directory.LOCAL_STORAGE, path=MOCKTAGE_PATH)\\\n",
    "        .add_file_servers(FileServer(\"file://\" + MOCKTAGE_PATH, Operation.ALL))\n",
    "    \n",
    "    condorpool.add_directories(condorpool_shared_scratch_dir, condorpool_shared_binary_dir)\n",
    "\n",
    "    # -----------------------------------------------                \n",
    "    # add the sites to the site catalog object\n",
    "    sc.add_sites(local, condorpool)\n",
    "\n",
    "    # write the site catalog to the default path \"./sites.yml\"\n",
    "    #set_trace()\n",
    "    sc.write()\n",
    "    \n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbeca20-5afc-49c7-8478-0a879c840985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_properties():\n",
    "    props = Properties() \n",
    "    #props[\"pegasus.mode\"] = \"development\"\n",
    "    #props[\"pegasus.data.configuration\"] = \"sharedfs\"\n",
    "    #props[\"pegasus.code.generator\"] = \"Shell\"\n",
    "    props.write()\n",
    "    return props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3069c30b-6adb-47e7-a4a0-838fbe3f930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_LEVEL = \"0\"\n",
    "        \n",
    "global_component_id = 0\n",
    "\n",
    "def next_id_string(): \n",
    "    global global_component_id\n",
    "    temp = global_component_id\n",
    "    global_component_id += 1\n",
    "    return 'ID-' + str(temp)\n",
    "\n",
    "def cdo_name(file):\n",
    "    return file # 'CDO-' + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d17932-7139-4236-8cf5-ebf3ebbc3e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDO:\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename    = filename\n",
    "        self.cached_name = 'cdo-cache-' + filename\n",
    "        self.input_count = 0\n",
    "        self.cache       = None\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Define a subclass of the pegasus workflow object \n",
    "#\n",
    "# Override pegasus job insertion, to customize for CDOs\n",
    "#\n",
    "class Maestro_Workflow(Workflow):\n",
    "    \n",
    "    def __init__(self, cdo_dependency, name: str, infer_dependencies: bool = True):\n",
    "        print(\"This is the init function\")\n",
    "        super().__init__(name, infer_dependencies)\n",
    "        self.parent_tasks = {}\n",
    "        self.cdo_dependency = cdo_dependency\n",
    "        self.pool_manager = Job(\"start-pool-manager.sh\", node_label=\"pool-manager\")\\\n",
    "                            .add_args(SCRATCH_PATH, \"pool_manager.stop\", MOCKTAGE_PATH + \"/pool_manager\", SCRATCH_PATH + \"/pminfo\") \\\n",
    "                            .add_metadata(maestro_workflow_core_backend=\"minio\", cdo_poolmanager='true')\n",
    "        super().add_jobs(self.pool_manager)\n",
    "        # .add_outputs(File(\"pool_io\"))\\\n",
    "\n",
    "    # find the input to a job that generates the named output\n",
    "    def find_parent_dependency(self, output):\n",
    "        for id, job in self.jobs.items():\n",
    "            # if this output matches the request, find the first input\n",
    "            for op in job.get_outputs():\n",
    "                #print('testing', op.lfn, 'against',output)\n",
    "                if op.lfn == output and len(job.get_inputs())>0:\n",
    "                    # just get the filename of the first input\n",
    "                    temp = next(iter(job.get_inputs())).lfn\n",
    "                    #print('Found a match using', temp)                    \n",
    "                    return next(iter(job.get_inputs())).lfn\n",
    "        print('No parent for', output)\n",
    "        return output\n",
    "\n",
    "    def is_watcher(self, job):\n",
    "        return ('cdo_watcher' in job.metadata)\n",
    "    \n",
    "    def is_cache(self, job):\n",
    "        return ('cdo_cache' in job.metadata)\n",
    "    \n",
    "    #\n",
    "    # This is the main routine that walks the graph and converts files to CDOs\n",
    "    # inserts watchers and cache objects.\n",
    "    #\n",
    "    def insert_cdo_jobs(self):        \n",
    "        # Show input output file objects to see which are shared objects\n",
    "        # for debugging only\n",
    "        if False:\n",
    "            for id, job in self.jobs.items():\n",
    "                for u in job.uses:\n",
    "                    #print(object.__repr__(u.file), u.file.lfn)\n",
    "                    ...\n",
    "                for ip in job.get_inputs():\n",
    "                    #print('Input', ip, ip.lfn)\n",
    "                    ...\n",
    "                for op in job.get_outputs():\n",
    "                    #print('Output', op, op.lfn)\n",
    "                    ...\n",
    "        \n",
    "        # note that we must rename input and outputs using new file objects\n",
    "        # to work around shared files that are both input and outputs\n",
    "        # and are replaced by CDO objects\n",
    "        i_replacements = {}\n",
    "        o_replacements = {}\n",
    "        \n",
    "        # store watchers created to prevent creating 2 watchers for the same CDO\n",
    "        # if it is consumed by more than one process\n",
    "        watchers   = {}\n",
    "        cdo_objs   = {}\n",
    "        extra_jobs = []\n",
    "        for id, job in self.jobs.items():                \n",
    "            # For each input :      in -> P -> out\n",
    "            #   replace with parent(in) -> watcher ->\n",
    "            #                                   -> (in)' -> P -> out\n",
    "            if len(job.get_inputs())>0:\n",
    "                for ip in job.get_inputs():\n",
    "                    cdo_enabled = True\n",
    "                    if \"cdo_disabled\" in ip.metadata:\n",
    "                        cdo_enabled = not ip.metadata['cdo_disabled'].lower() in ['true', '1', 't', 'y', 'yes']                    \n",
    "                        #print(ip, \"is cdo enabled\", cdo_enabled)\n",
    "                    if not cdo_enabled:\n",
    "                        print('No substitution for non CDO enabled input', ip.lfn)\n",
    "                        continue\n",
    "                    \n",
    "                    ip_name        = ip.lfn\n",
    "                    trigger_name   = 'T-' + ip_name\n",
    "                    node_label     = '' + ip_name\n",
    "                    cache_label    = '' + ip_name\n",
    "                    \n",
    "                    # track how many consumers are taking this CDO as an input\n",
    "                    if not ip_name in cdo_objs:\n",
    "                        \n",
    "                        cdo_objs[ip_name] = CDO(ip.lfn)\n",
    "                        cdo_objs[ip_name].input_count = 1\n",
    "                        \n",
    "                        # if multiple processes consume the same CDO, we only need one watcher\n",
    "                        # create a watcher for this CDO input\n",
    "                        id_string = next_id_string()\n",
    "                        watcher = Job(\"process-CDO\", node_label = id_string)\n",
    "                        pseudo_parent = self.find_parent_dependency(ip_name)\n",
    "                        watcher.add_env(MSTRO_LOG_LEVEL=LOG_LEVEL)\n",
    "                        watcher.add_inputs(pseudo_parent)\n",
    "                        watcher.add_outputs(File(trigger_name).add_metadata(dummy_file='true'), stage_out=True)\n",
    "                        watcher.add_args('-l', SCRATCH_PATH,     # log directory \n",
    "                                         '-p', 'pminfo',         # pool manager info\n",
    "                                         '-t', trigger_name,     # trigger_file for pegasus\n",
    "                                         '-c', id_string,        # component name, must be unique\n",
    "                                         '-i', ip_name)          # list of input CDOs to consume\n",
    "                        watcher.add_metadata(cdo_watcher='true')\n",
    "                        watchers[node_label] = watcher\n",
    "                        \n",
    "                        id_string = next_id_string()\n",
    "                        #cache_label = id_string # REMOVE AFTER DEBUGGING\n",
    "                        cache = Job(\"process-CDO\", node_label = id_string) # cache_label)\n",
    "                        cache.add_env(MSTRO_LOG_LEVEL=LOG_LEVEL)\n",
    "                        cache.add_inputs(pseudo_parent) \n",
    "                        cache.add_args('-l', SCRATCH_PATH,     # log directory \n",
    "                                       '-p', 'pminfo',         # pool manager info\n",
    "                                       '-c', id_string,        # component name, must be unique\n",
    "                                       '-g',                   # stager mode, copies in to out (cache)\n",
    "                                       '-i', ip_name)          # list of input CDOs to consume\n",
    "                        cache.add_metadata(cdo_cache='true')\n",
    "\n",
    "                        # add the cache object for lookup later\n",
    "                        cdo_objs[ip_name].cache = cache\n",
    "                        \n",
    "                        # Add these new jobs to the actual DAG\n",
    "                        extra_jobs.append(watcher)\n",
    "                        extra_jobs.append(cache)\n",
    "                        \n",
    "                    else:\n",
    "                        print('Adding to', ip_name) \n",
    "                        cdo_objs[ip_name].input_count += 1\n",
    "                        \n",
    "                    # any process that outputs this data will need to rename it to the new input name\n",
    "                    o_replacements[ip_name] = ip_name\n",
    "                    \n",
    "                    # for dependencies that use files as input : rename it to the new trigger file name \n",
    "                    i_replacements[ip_name] = trigger_name\n",
    "                \n",
    "            if \"final_job\" in job.metadata:\n",
    "                print ('final job', id, 'corresponds to', job.node_label)\n",
    "                newjob = Job(\"stop-pool-manager.sh\", node_label=\"stop-pool-manager\")\n",
    "                newjob.add_args(SCRATCH_PATH, 'pool_manager.stop') \\\n",
    "                    .add_metadata(cdo_poolmanager='true')\n",
    "                for op in job.get_outputs():\n",
    "                    newjob.add_inputs(op.lfn)\n",
    "                extra_jobs.append(newjob)\n",
    "\n",
    "        for job in extra_jobs:\n",
    "            if job._id is None:\n",
    "                job._id = self._get_next_job_id()\n",
    "            self.jobs[job._id] = job\n",
    "        \n",
    "        # when we replace an input to a job with a CDO version of it, we have to create a new \"File\" object\n",
    "        # because if we simply change the path/name, we might modify the same 'file' object on different \n",
    "        # jobs and we can get links between tasks we were not expecting\n",
    "        for id, job in self.jobs.items():\n",
    "            for u in job.uses:\n",
    "                if u.file.lfn in i_replacements:\n",
    "                    # Replace an input that we have changed to point to the dummy file\n",
    "                    if u._type == \"input\":\n",
    "                        u.file = File(i_replacements[u.file.lfn]).add_metadata(dummy_file='true')\n",
    "                    # Replace an output that we have changed to point to the CDO\n",
    "                    if u._type == \"output\":\n",
    "                        # we add a watcher and a cache as dependencies of this CDO\n",
    "                        job.add_args('-O', '2')\n",
    "                        # make sure the CDO cache is kept alive for N real consumers\n",
    "                        cdo_objs[u.file.lfn].cache.add_args('-O', cdo_objs[u.file.lfn].input_count)\n",
    "                        if u.file.lfn in cdo_objs:\n",
    "                            if self.cdo_dependency :\n",
    "                                u.file = File(o_replacements[u.file.lfn]).add_metadata(cdo_data='true') \n",
    "                            else:\n",
    "                                u.file = None\n",
    "            job.uses = [x for x in job.uses if x.file is not None]\n",
    "                                    \n",
    "        print('Substitution of command line filenames for cached CDOs')\n",
    "        for id, job in self.jobs.items():\n",
    "            # watchers always watch for the original CDO (no name change)\n",
    "            # cache's will output a CDO with a new name (name change handled by cache itself)\n",
    "            # other objects must rename their inbput file/cdo names to the renamed version\n",
    "            if self.is_watcher(job) or self.is_cache(job):\n",
    "                ...\n",
    "            else:\n",
    "                new_args = []\n",
    "                # we must only change input CDO names, as original output names go into the cache \n",
    "                input = False\n",
    "                for a in job.args:\n",
    "                    if a == '-i':\n",
    "                        input = True\n",
    "                    if a == '-o':\n",
    "                        input = False\n",
    "                    if input and isinstance(a, File):\n",
    "                        cdo_name = 'cdo-cache-' + a.lfn\n",
    "                        a = File(cdo_name)\n",
    "                    new_args.append(a)\n",
    "                job.args = new_args\n",
    "            #\n",
    "            print(job.args)\n",
    "                    \n",
    "        for d, val in self.dependencies.items():\n",
    "            print('Dependency', d, val)\n",
    "\n",
    "            \n",
    "    def execute_using_slurm(self):\n",
    "        return        \n",
    "    \n",
    "    def build_dependencies(self):\n",
    "        # buiod list of children for each task\n",
    "        self.infer_dependencies = True\n",
    "        self._infer_dependencies()\n",
    "        \n",
    "        # construct list of parents from child list\n",
    "        for k,v in self.dependencies.items():\n",
    "            for c in v.children_ids:\n",
    "                if c in self.parent_tasks:\n",
    "                    self.parent_tasks[c] = self.parent_tasks[c] + [k]\n",
    "                else:\n",
    "                    self.parent_tasks[c] = [k]\n",
    "                    \n",
    "        for task, parent in self.parent_tasks.items():\n",
    "            #print('Task', task, 'Depends on', parent)\n",
    "            ...\n",
    "        return\n",
    "        \n",
    "    def execute_using_splinter(self):\n",
    "        # get the transformation catalog\n",
    "        #print(dir(self.transformation_catalog))\n",
    "        tc = self.transformation_catalog.transformations        \n",
    "        # for x in tc:\n",
    "        #     print (x)\n",
    "            \n",
    "        # build parent/child dependency lists \n",
    "        self.build_dependencies()\n",
    "        # create a splinter workflow\n",
    "        swf = splinter_workflow()\n",
    "        \n",
    "        for id, job in self.jobs.items():\n",
    "            t_string = \"None::\" + job.transformation + \"::None\"\n",
    "            t_path = tc[t_string].sites['local'].pfn            \n",
    "            # convert any file objects to string pathnames in arg list\n",
    "            command = [t_path] + [str(a) for a in job.args]\n",
    "            parents = self.parent_tasks[id] if id in self.parent_tasks else []            \n",
    "            splinter_task = task(id, command, parents)\n",
    "            swf.add_task(splinter_task)\n",
    "            \n",
    "        swf.execute_workflow(64, 2)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9d627-cbe0-4fa9-a824-1960de781284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def regex_increment_first(instring):\n",
    "    # preceeded by \"-\" : followed by \"-\"\n",
    "    out = re.sub('(?<=-)(\\d+)(?=-)', lambda x: str(int(x.group(0)) + 1).zfill(2), instring)\n",
    "    return out\n",
    "\n",
    "def regex_increment_last(instring):\n",
    "    # preceeded by \"-\" : followed by EOL\n",
    "    out = re.sub('(?<=-)(\\d+$)', lambda x: str(int(x.group(0)) + 1).zfill(2), instring)\n",
    "    return out\n",
    "\n",
    "x = \"f-04-05\"\n",
    "print(regex_increment_first(x))\n",
    "print(regex_increment_last(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64da5f10-aea4-493f-88e0-14fb4f839ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demo_workflow(wf, rc, maestro=False, iterations=2, forks=2, subforks=2):\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Create a single input file that will start our graph\n",
    "    fa = File(\"root-data.txt\").add_metadata(creator=\"biddisco\", cdo_disabled=\"true\", maestro_enabled=\"false\", node_label='root')\n",
    "    rc.add_replica(\n",
    "       site=\"local\", lfn=fa, pfn=Path(DATA_PATH).resolve() / \"root-data.txt\"\n",
    "    )\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # Create a single job that will fork into N new files\n",
    "    files = []\n",
    "    for f in range(0,forks):\n",
    "        # create string \"f-0N-00\"\n",
    "        files.append(File(\"f-\" + f\"{f:0>2}\" + \"-00\"))\n",
    "    \n",
    "    id_string = next_id_string()\n",
    "    node_label = \"preprocess\"\n",
    "    job_preprocess = Job(\"process-CDO\", node_label=id_string   )      \\\n",
    "                            .add_env(MSTRO_LOG_LEVEL=LOG_LEVEL)       \\\n",
    "                            .add_inputs(fa)                           \\\n",
    "                            .add_outputs(*files, stage_out=True)      \\\n",
    "                            .add_metadata(node_colour='#e959d9')      \\\n",
    "                            .add_args('-l', SCRATCH_PATH,           # log directory \n",
    "                                      '-p', 'pminfo',               # pool manager info\n",
    "                                      '-c', id_string,              # component name, must be unique\n",
    "                                      '-o', *[x for x in files])    # list of output CDOs to produce\n",
    "                                    #'-O', *(['1']*forks))         # all outputs have '1' consumer \n",
    "    print('args are', job_preprocess.args) \n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # for each fork, produce a chain of iterations file_in->P->file_out processes\n",
    "    job_iter = []   \n",
    "    for i in range(0, iterations):\n",
    "        out_files = []\n",
    "        for f in range(0,forks):\n",
    "                \n",
    "            in_name = files[f].lfn\n",
    "            #print('input name', in_name)\n",
    "            out_name = regex_increment_last(in_name)\n",
    "            #print('output iteration/fork', in_name)\n",
    "            f_out = File(out_name)            \n",
    "            out_files.append(f_out)\n",
    "\n",
    "            # on first iteration, add an extra fork+join to test our CDO stuff\n",
    "            if i==0 and subforks>1:\n",
    "                subfiles = []\n",
    "                # create a set of tasks that fork from a single input\n",
    "                for sf in range(0,subforks):\n",
    "                    sf_out = File(out_name + \"-\" + str(sf))\n",
    "                    subfiles.append(sf_out)\n",
    "                    id_string = next_id_string()\n",
    "                    node_label = sf_out.lfn\n",
    "                    forkjob = Job(\"process-CDO\", node_label=id_string)\\\n",
    "                                .add_metadata(node_colour='#1b9e77') \\\n",
    "                                .add_env(MSTRO_LOG_LEVEL=LOG_LEVEL)  \\\n",
    "                                .add_inputs(files[f])                \\\n",
    "                                .add_outputs(sf_out, stage_out=True) \\\n",
    "                                .add_args('-l', SCRATCH_PATH,     # log directory \n",
    "                                          '-p', 'pminfo',         # pool manager info\n",
    "                                          '-c', id_string,        # component name, must be unique\n",
    "                                          '-i', files[f],         # (list of) input CDO(s) to consume\n",
    "                                          '-o', sf_out)           # output (default 1 consumer omitted)\n",
    "                    job_iter.append(forkjob)\n",
    "\n",
    "                # join all the tasks back into a single output    \n",
    "                id_string = next_id_string()\n",
    "                node_label = str(f)+\"-process-\" + str(i)\n",
    "                joinjob = Job(\"process-CDO\", node_label=id_string)\\\n",
    "                                .add_metadata(node_colour='#3b97be')\\\n",
    "                                .add_env(MSTRO_LOG_LEVEL=LOG_LEVEL) \\\n",
    "                                .add_inputs(*subfiles)              \\\n",
    "                                .add_outputs(f_out, stage_out=True) \\\n",
    "                                .add_args('-l', SCRATCH_PATH,           # log directory \n",
    "                                          '-p', 'pminfo',               # pool manager info\n",
    "                                          '-c', id_string,              # component name, must be unique\n",
    "                                          '-i', *[x for x in subfiles], # (list of) input CDO(s) to produce\n",
    "                                          '-o', f_out)                  # output (default 1 consumer omitted)                \n",
    "                job_iter.append(joinjob)\n",
    "                \n",
    "            else:\n",
    "                id_string = next_id_string()\n",
    "                node_label = str(f)+\"-process-\" + str(i)\n",
    "                job_iter.append(Job(\"process-CDO\", node_label=id_string)\\\n",
    "                                .add_metadata(node_colour='#ff7fb3')\\\n",
    "                                .add_env(MSTRO_LOG_LEVEL=LOG_LEVEL) \\\n",
    "                                .add_inputs(files[f])               \\\n",
    "                                .add_outputs(f_out, stage_out=True) \\\n",
    "                                .add_args('-l', SCRATCH_PATH,           # log directory \n",
    "                                          '-p', 'pminfo',               # pool manager info\n",
    "                                          '-c', id_string,              # component name, must be unique\n",
    "                                          '-i', files[f],               # list of input CDOs to produce\n",
    "                                          '-o', f_out))                 # output (default 1 consumer omitted)                \n",
    "                                \n",
    "        files = out_files\n",
    "\n",
    "    fd = File(\"f.o\").add_metadata(final_output=\"true\", cdo_disabled=\"true\")\n",
    "    id_string = next_id_string()\n",
    "    node_label = \"analyze\"\n",
    "    job_analyze = Job(\"process-CDO\", node_label=id_string)                \\\n",
    "                    .add_env(MSTRO_LOG_LEVEL=\"3\")                         \\\n",
    "                    .add_inputs(*files)                                   \\\n",
    "                    .add_outputs(fd, stage_out=True)                      \\\n",
    "                    .add_metadata(final_job='true', node_colour='#8a4f4f')\\\n",
    "                    .add_args('-l', SCRATCH_PATH,           # log directory \n",
    "                              '-p', 'pminfo',               # pool manager info\n",
    "                              '-c', id_string,              # component name, must be unique\n",
    "                              '-i', *[x for x in files],    # list of input CDOs to produce\n",
    "                              '-t', fd.lfn)                 # output (default 1 consumer omitted)                \n",
    "\n",
    "    wf.add_jobs(job_preprocess, job_analyze)\n",
    "    for j in job_iter:\n",
    "        wf.add_jobs(j)\n",
    "\n",
    "    if maestro:\n",
    "        wf.insert_cdo_jobs()\n",
    "        \n",
    "    wf.add_replica_catalog(rc)\n",
    "    wf.write(file=wf.name)\n",
    "    return wf.path.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614350f2-2ad0-410a-91ac-fcb12d571b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pegasus_graphviz)\n",
    "\n",
    "# rrdl = remove redundant dependency links\n",
    "def display_workflow(workflow_file, rrdl, display_files, leftright):\n",
    "    #print(dir(workflow_file))\n",
    "    dot_file = Path(workflow_file).with_suffix('.dot')\n",
    "    # invoke emit_dot on the workflow : show file input/outputs\n",
    "    dag = pegasus_graphviz.parse_yamlfile(workflow_file, include_files=display_files)\n",
    "    \n",
    "    #pegasus_graphviz.remove_xforms(dag, 'L_0')\n",
    "    \n",
    "    if rrdl:\n",
    "        # remove redundant dependency links\n",
    "        dag = pegasus_graphviz.transitivereduction(dag)\n",
    "\n",
    "    # labeloptions = [\"label\", \"xform\", \"id\", \"xform-id\", \"label-xform\", \"label-id\"]\n",
    "    dot = pegasus_graphviz.emit_dot(dag, label_type='label', outfile=dot_file, leftright=leftright)\n",
    "\n",
    "    s = graphviz.Source.from_file(dot_file)\n",
    "    image_file = graphviz.render(filepath=dot_file, engine='dot', format='pdf')\n",
    "    return image_file # Image(filename=image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b257950-ff74-450e-8210-e3a397dfcdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# cdo_dependencies : false, CDOs are not matched between in/out so CDO consumers do not depend on producers, the DAG is split\n",
    "#                  : true, CDOs behave like files and trigger dependencies\n",
    "# display_files : true, files appear as nodes in the graph, otherwise not\n",
    "# transitive_reduction : true - remove links that are superfluous - transitive, between job-job bypassing files\n",
    "\n",
    "# cdo_dependencies must be False when executing a CDO enabled workflow\n",
    "\n",
    "cdo_dependencies     = False\n",
    "display_files        = True\n",
    "transitive_reduction = True\n",
    "left_right           = True\n",
    "\n",
    "global_component_id = 1000\n",
    "\n",
    "if os.path.isfile(\"pegasus.properties\"):\n",
    "    os.remove(\"pegasus.properties\")\n",
    "if os.path.isfile(\"sites.yml\"):\n",
    "    os.remove(\"sites.yml\")\n",
    "\n",
    "rco = ReplicaCatalog()\n",
    "rcm = ReplicaCatalog()\n",
    "sco = build_site_catalog()    \n",
    "prp = build_properties()\n",
    "\n",
    "# ---------------------------------------\n",
    "# Generate workflows, one original, one maestro enabled\n",
    "wfo = Workflow(name=\"demo-orig.yml\")\n",
    "wfm = Maestro_Workflow(cdo_dependencies, name=\"demo-maestro.yml\", infer_dependencies=False)\n",
    "\n",
    "build_transformation_catalog(wfo)\n",
    "build_transformation_catalog(wfm)\n",
    "    \n",
    "# ---------------------------------------\n",
    "# Convert workflow into nice DAG display\n",
    "iterations = 3\n",
    "forks = 3\n",
    "subforks = 2\n",
    "file1 = generate_demo_workflow(wfo, rco, iterations=iterations, forks=forks, subforks=subforks)\n",
    "I1 = display_workflow(file1, transitive_reduction, display_files, left_right)\n",
    "\n",
    "file2 = generate_demo_workflow(wfm, rcm, maestro=True, iterations=iterations, forks=forks, subforks=subforks)\n",
    "I2 = display_workflow(file2, transitive_reduction, display_files, left_right ) # left_right and cdo_dependencies)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# rename pdf files based on what options we used to generate the DAG pics\n",
    "fstring  = \"-cdo\" if cdo_dependencies else \"\"\n",
    "fstring += \"-files\" if display_files else \"\"\n",
    "fstring += \"-reduced\" if transitive_reduction else \"\"\n",
    "image_name1 = I1.replace('.dot.pdf', fstring + '.pdf')\n",
    "image_name2 = I2.replace('.dot.pdf', fstring + '.pdf')\n",
    "os.rename(I1, image_name1)\n",
    "os.rename(I2, image_name2)\n",
    "\n",
    "# ---------------------------------------\n",
    "# read+convert PDFs into images\n",
    "img_A = WImage(filename=image_name1)\n",
    "img_B = WImage(filename=image_name2)\n",
    "\n",
    "# ---------------------------------------\n",
    "# figure size in inches optional\n",
    "rcParams['figure.figsize'] = 32, 32\n",
    "# ---------------------------------------\n",
    "# display images in notebook\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.imshow(img_A);\n",
    "ax.set_axis_off()\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.imshow(img_B);\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363020c-ae67-4dc3-af1f-77bd98113347",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfm.execute_using_splinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e2493-5457-44bd-b664-1574ea05c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wfo.plan(submit=True, sites=['condorpool'], cleanup=False)\\\n",
    "# wfm.plan(submit=True, cleanup=False, sites=[\"condorpool\"],verbose=0)\\\n",
    "#     .wait()\\\n",
    "#     .analyze()\\\n",
    "#     .statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e61cb-3aea-4c30-8c00-d5f8d1fb2e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wfm.halt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bbcdec-aa7b-4ab0-8434-b0489e5859a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
